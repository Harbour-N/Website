[
  {
    "objectID": "Pubs/my_first_pub/index.html",
    "href": "Pubs/my_first_pub/index.html",
    "title": "CCRG-01. INFERENCE OF CELL CYCLE REGULATION BETWEEN GLIOBLASTOMA SUBPOPULATIONS IN VIVO TO DRIVE COMPUTATIONAL AND MATHEMATICAL MODELS OF THE CANCER COMPLEX SYSTEM",
    "section": "",
    "text": "Abstract\nGlioblastoma (GBM) is the most aggressive and most common primary malignant brain tumor in adults, with a poor median survival time of 15 months. One of the key challenges in treating GBM is its highly heterogeneous nature, with multiple distinct subtypes that have been shown to occur on both inter- and intra-patient levels. Three main classifications, known as classical/proliferative, mesenchymal and proneural have become commonly demonstrated phenotypes. The cell cycle is a fundamental and highly conserved process that controls faithful division of cells; dysregulation of the cell cycle is known to be a key driver in many cancers. However, how the cell cycle is differently regulated between these subtypes has not been well classified in vivo. We investigate these three GBM subtypes using a recently published single nucleus RNAseq (snRNAseq) data set. We compare cell cycle regulation/dysregulation among these three subtypes using Tricycle, an R/Bioconductor package that utilises dimension reduction via principal component analysis and transfer learning to infer cell cycle position from any snRNAseq data set. We find that the classical GBM subtype has the highest proportion of actively dividing cells (cells in: S/G2/M phases), while the mesenchymal and proneural subtypes have a very low proportion of actively dividing cells. This supports the idea of a proliferation-migration dichotomy between GBM subtypes. We use this proportion of actively proliferating cells to calibrate a minimal spatiotemporal mathematical model for GBM tumor growth that accounts for the differences in cell cycle regulation between these three GBM subtypes."
  },
  {
    "objectID": "posts/Perlin_noise/index.html",
    "href": "posts/Perlin_noise/index.html",
    "title": "The Mathematics of Perlin noise",
    "section": "",
    "text": "Perlin noise is a versatile and widely-used algorithm in computer graphics and simulation, renowned for generating natural-looking patterns and smooth transitions. Perlin developed his algorithm in the 1980s while working on Disney’s animated film Tron (1982), in order to improve the realism of computer generated imagery (CGI) at the time. Perlin received the Academy Award for Technical Achievement for his algorithm, who knew that mathematician/computer sicentists win Oscars too!\nThis post is inspired by an article in the IMA’s mathematics today [1], which focused on the mathematics used in video games. In this post I follow the authors description of Perlins algorithm, and provide example code to implement this algorithm in Python using only Numpy (and Matplotlib for visualisation).\nAdd a Pic of normal noise compared to Perlin noise"
  },
  {
    "objectID": "posts/Perlin_noise/index.html#step-1",
    "href": "posts/Perlin_noise/index.html#step-1",
    "title": "The Mathematics of Perlin noise",
    "section": "Step 1",
    "text": "Step 1\nFirst we need to define a set of regularly spaced points (nodes) along a line. Below shows an example where \\(i=11\\) nodes.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Step 1) Form a set of regularly spaced points (called nodes)\nnodes = np.linspace(0, 10,11)\ny = np.zeros(len(nodes))\n\nplt.axhline(c= 'k', alpha = 0.5)\nplt.plot(nodes,y, 'b*', markersize=10)\nplt.title(\"Define nodes\")\nplt.xticks(nodes)\nplt.yticks([])\nplt.show()"
  },
  {
    "objectID": "posts/Perlin_noise/index.html#step-2",
    "href": "posts/Perlin_noise/index.html#step-2",
    "title": "The Mathematics of Perlin noise",
    "section": "Step 2",
    "text": "Step 2\nAt each node we define a random vector with gradient \\(m_i\\) (where \\(i = 1,2, …, 10\\)). For this article we will follow [1] arbitrarily take \\(-3 \\leq m_i \\leq 3\\).\n\n\nCode\n# Step 2) At each node generate a random vector with\n# gradient m_i, arbritrarily set -3&lt;= m_i &lt;= 3\na = -3\nb = 3\nm = (b-a)*np.random.random(len(nodes)) + a\n\n# Define a function to visualise the random gradients\ndef plot_grad(node,grad, spacing):\n    area_around_node = np.linspace(node-spacing,node+spacing,11)\n    y = grad*(area_around_node - node)\n    return area_around_node, y\n\nfor i in range(0,len(nodes)):\n    w, s = plot_grad(nodes[i],m[i], 0.1)\n    plt.plot(w,s,'r-', alpha = 0.7)\n\nplt.axhline(c= 'k', alpha = 0.5)\nplt.plot(nodes,np.zeros(len(nodes)), 'b*', markersize=10)\nplt.title(\"Define random gradients\")\nplt.xticks(nodes)\nplt.yticks([])\nplt.show()"
  },
  {
    "objectID": "posts/Perlin_noise/index.html#step-3",
    "href": "posts/Perlin_noise/index.html#step-3",
    "title": "The Mathematics of Perlin noise",
    "section": "Step 3",
    "text": "Step 3\nIf we consider an arbitrary point \\(x\\) in within the interval \\([p,r]\\) (in the example below we consider the first interval \\([0,1]\\)) and want to determine the point where the randomly generated vectors would intercept point \\(x\\). If we denote \\(A\\) the point where the random vector from the lower boundary of the interval will intercept then \\(A = (x, mₚ(x - p))\\) and the point \\(B\\) where the random vector from the upper node of the interval will intercept the point \\(x\\) is given by \\(B = (x, - mᵣ(r- x))\\). To simplify notation we will write \\(A = (x, n_p)\\) and \\(B = (x, n_r)\\) where we have taken: \\(n_p = m_p(x - p)\\), and \\(n_r = - m_r(r - x)\\).\n\n\nCode\n# Step 3) consider a point x where x is between 0,1 (i.e. between our 1st and 2nd node)\n# the points A = (x,m_0x), B = (x, -m_1 (1-x))\n\n# we will abritraily take x = 0.6 for this visualisation\nx = 0.6\n\nplt.axhline(c= 'k', alpha = 0.5,label='_nolegend_')\nplt.axvline(x,c= 'k', alpha = 0.5,label='_nolegend_')\nplt.plot(nodes[0],0, 'b*',label='_nolegend_')\nplt.plot(nodes[1],0, 'b*',label='_nolegend_')\nfor i in range(0,2):\n    w, s = plot_grad(nodes[i],m[i], 0.7)\n    plt.plot(w,s,'r-', alpha = 0.7,label='_nolegend_')\n\nA = m[0]*x\nB = -m[1]*(1-x)\n\nplt.plot(x, A,'gD')\nplt.plot(x,B, 'mD')\nplt.legend([\"A\",\"B\"])\nplt.xlim([-0.25,1.25])\n\nplt.xticks([0,x,1],labels=[\"0\", \"x\", \"1\"])\nplt.yticks([])\n# make a multicolored title \nplt.figtext(0.445, 0.92, \"Visualise points\", fontsize='large', color='k', ha ='center')\nplt.figtext(0.56, 0.92, \"A\", fontsize='large', color='g', ha ='right')\nplt.figtext(0.62, 0.92, \"B\", fontsize='large', color='m', ha ='left')\nplt.figtext(0.59, 0.92, ' and ', fontsize='large', color='k', ha ='center')\nplt.show()"
  },
  {
    "objectID": "posts/Perlin_noise/index.html#step-4",
    "href": "posts/Perlin_noise/index.html#step-4",
    "title": "The Mathematics of Perlin noise",
    "section": "Step 4",
    "text": "Step 4\nThis is the key step. In order to create smooth more natural looking noise we use interpolation to find the value of the noise function at point x (i.e. noise function between nodes). Originally, Perlin used cubic interpolation function: \\(f(x) = 3x² - 2x³\\), and we will use this in the following (although he later changed it to a quintic function: \\(f(x) = 6x^5 - 15x^4 - 10x^3\\) ). This interpolation function ensures that the derivative of the noise function is continuous at every node, which is what gives Perlin noise its more natural look. The noise function between two nodes is the calculated as:\n\\(\\text{noise}(x) = n_p + (n_r - n_p)f \\left( \\frac{x-r}{p-r} \\right) = n_p \\left(1 - f \\left( \\frac{x-r}{p-r} \\right) \\right) + n_r f \\left( \\frac{x-r}{p-r} \\right)\\)\nWe note that \\(\\text{noise}(p) = \\text{noise}(r) = 0\\), this is a general property of the noise function and ensures continuity at each node. We also note that we have scaled the noise function according to the size and starting point of the interval, this ensures that the noise function is consistent between different interval sizes and starting points. Applying steps 1–4 across all the nodes we can create a different noise function between each interval. When we combine all the noise functions across the intervals together (concatenate them) we produce what is known as a single octave of Perlin noise. For a particular octave and interval we denote the noise function as \\(\\text{noise}_s,t(x)\\). Then for a particular octave (\\(s\\)) the combination of noise functions across all interval is given by\n\\(\\text{Noise}_s(x) = \\sum_{t=1}^{t=7} \\text{noise}_{s,t}(x)\\). \nIn order to produce more realistic/natural looking images we combine several octaves of Perlin noise. This is achieved by using a vertical scaling factor and by increasing the number of nodes in each successive octave. Thus octave two will have \\(20\\) nodes, where the first interval will now be \\([0,0.5]\\) and the scaling factor will therefore be \\(0.5\\). Below I have plotted the fisrt two octaves of Perlin noise.\n\n\nCode\n# Step 4) use interpolation to find the value of the noise function at x\n# we will use the cubic interpolation function as in original Perlin noise\n# f(x) = 3x^2 - 2x^3\n# can also use quintic interpolation function: f(x) = \n\n# Interpolation function\ndef f(x):\n    return 3.0*x*x - 2.0*x*x*x\n\n# Calulate noise function\ndef noise(interval,m0,m1,dx):\n    '''\n\n    Parameters\n    ----------\n    interval : numpy array of float64 (vector)\n        Begining and end nodes of interval.\n    m0 : float64\n        The random gradient at the begingin node.\n    m1 : float64\n        The random gradient at the end node.\n    dx : int\n        The number of intermediate values between the begining and end node \n        at which to calculare the noise function.\n\n    Returns\n    -------\n    noise_return : numpy array of float64 (vector)\n        The value of the noise function at intermediate points \n        between beginig and end node\n    x : numpy array of float64 (vector)\n        The intermediate locations between beignig and end node at which the noise\n        function was calulated.\n\n    '''\n    intStart = interval[0]\n    intEnd = interval[-1]\n    intSize = intEnd - intStart\n    x = np.linspace(interval[0], interval[1],  int(np.round(dx * intSize)))\n    #x = np.arange(interval[0],interval[1],dx)\n    n0 = m0*(x-interval[0] )\n    n1 = -m1*(interval[1] - x)\n    noise_return = n0*(1.0-f( (1/intSize)*(x-intStart))) + n1*f((1/intSize)*(x-intStart))\n    return noise_return, x\n\ndx = 100\n\n# calculate noise function between all nodes\nNOISE = np.array([]) # numpy array to store noise function accross all intervals\nX = np.array([]) # numpy array to store the x value of the noise function between all nodes\nfor i in range(0,len(nodes)-1):\n    q,t = noise([nodes[i],nodes[i+1]],m[i],m[i+1],dx)\n\n    NOISE = np.append(NOISE,q)\n    X = np.append(X,t)\n\n# set up a subplot to plot the 1st and 2nd octave side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n\n# plot First octave of noise\nax1.axhline(c= 'k', alpha = 0.5) \nax1.plot(X,NOISE)\nax1.set_xticks(nodes)\nax1.set_yticks([])\nax1.set_title(\"First octave\")\n\n# add the original random gradients to the plot\nfor i in range(0,len(nodes)):\n    w, y = plot_grad(nodes[i],m[i], 0.1)\n    ax1.plot(w,y,'r-', alpha = 0.7)\n\n# the first octave has a frequecy of 10 and a scaling of 1\n# Octave 2 will have a frequency of 20 so [0,0.5] will be the first interval\n\noctave_num = 2\noctave2 = np.linspace(nodes[0], nodes[-1], len(nodes)*octave_num-1)\nm = (b-a)*np.random.random(len(octave2)) + a\n\n# calculate noise for all nodes\nNOISE2 = np.array([]) # noise function accross all intervals\nX2 = np.array([])\nfor i in range(0,len(octave2)-1):\n    q,t = noise([octave2[i],octave2[i+1]],m[i],m[i+1],dx)\n\n    NOISE2 = np.append(NOISE2,q)\n    X2 = np.append(X2,t)\n    \nax2.axhline(c= 'k', alpha = 0.5)\nax2.plot(X2,NOISE2)\nax2.set_xticks(octave2)\nax2.set_yticks([])\nax2.set_title(\"Second octave\")\n\nfor i in range(0,len(octave2)):\n    w, y = plot_grad(octave2[i],m[i], 0.05)\n    ax2.plot(w,y,'r-', alpha = 0.7)\n    \nplt.show()"
  },
  {
    "objectID": "posts/Perlin_noise/index.html#step-5",
    "href": "posts/Perlin_noise/index.html#step-5",
    "title": "The Mathematics of Perlin noise",
    "section": "Step 5",
    "text": "Step 5\nIn general we can write the combination of several octaves (where we assume the scaling factor has not yet been applied) of noise as \n\\(equation = goeshearr 1+6\\)\nLets combine the first two octaves of Perlin noise that we previously calculated.\n\n\nCode\n# Step 5) compute the weighted sum of two Octaves\n\n# finally combine the two octaves\nsumOctaves = NOISE + 0.5*NOISE2\n\nplt.axhline(c= 'k', alpha = 0.5)\nplt.plot(X2,sumOctaves)\nplt.xticks(nodes)\nplt.yticks([])\nplt.title(\"Sum of first 2 octaves\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nIn practice we often combine 6 or more octaves. To do this we will write a function that combines all the previous steps and allows us to combine as many octaves as we wish.\n\n\nCode\n# Step 5) compute the weighted sum of T=6 octaves\n# to do this we will write a function that performs all the previous steps\n# and can then combine as many octaves as we want\n\ndef PerlinNoise1D(interval,numNodes,numOctaves,dx):\n    '''\n\n    Parameters\n    ----------\n    interval : numpy array float64 (vector)\n        beging and end of interval on which to produce Perlin noise.\n    numNodes : int\n        Number of nodes to use.\n    numOctaves : int\n        Number of Octaves of Perlin noise to combine\n    dx : int\n        Number of intermediate points along x axis at \n        which to calculate the noise function.\n\n    Returns\n    -------\n    Noise_sum : numpy array float64 (vector)\n        The sum of all octaves of noise function.\n    T : numpy array float64 (vector)\n        The x values at which the noise function has been calculated.\n\n    '''\n \n    intSize = interval[1] - interval[0]\n    Noise_sum = np.zeros(int(np.round(dx * intSize)))\n    \n    for n in range(1,numOctaves+1):\n        \n        #print(\"Octave number = \" + str(n))\n        Noise_current = np.array([]) # store noise function accross all intervals for a single octave\n        T = np.array([])\n        \n        # Step 1) For a set of regularly spaced points (called nodes)\n        if n == 1 :\n            nodes = np.linspace(interval[0], interval[1], numNodes)\n            current_size = len(nodes)\n        else:\n            nodes = np.linspace(interval[0], interval[1], current_size*2 -1)\n            current_size = len(nodes)\n\n        # Step 2) at each node generate a random vector with\n        # gradient m_i, arbritrarily set -3&lt;= m_i &lt;= 3\n        a = -3\n        b = 3\n        m = (b-a)*np.random.random(len(nodes)) + a\n        \n        # Step 4) calculate the noise function\n        for i in range(0,len(nodes)-1):\n            q,t = noise([nodes[i],nodes[i+1]],m[i],m[i+1],dx)\n\n            Noise_current = np.append(Noise_current,q)\n            T = np.append(T,t)\n    \n        # Step 5) calcualte the weighted sum of octaves\n        Noise_sum = Noise_sum + (1 / (2**(n-1))) * Noise_current\n   \n    return Noise_sum, T\n\n# input parameters \ninterval = [0,10]\nnumNodes =11;\nnumOctaves = 6\ndx = 100000 #reduce this if taking a long time\ntest, t = PerlinNoise1D(interval, numNodes, numOctaves, dx)\n\nplt.figure()\nplt.axhline(c= 'k', alpha = 0.5)\nplt.plot(t,test)\nplt.title(str(numOctaves) + \" Octaves of Perlin noise\")\nplt.xticks(nodes)\nplt.yticks([])\n\n\n\n\n\n\n\n\n\nCombining more octaves produces more fractal-like noise as finer structures are successively added."
  },
  {
    "objectID": "posts/Perlin_noise/index.html#step-1-1",
    "href": "posts/Perlin_noise/index.html#step-1-1",
    "title": "The Mathematics of Perlin noise",
    "section": "Step 1",
    "text": "Step 1\nSimilarlly to the 1D case we start by generating a set of nodes, in the 2D case this will now be a grid of nodes. At each node we generate a random vector. Figure 1\n\n\nCode\nx = np.arange(0,4,1)\ny = np.arange(0,4,1)\n\n# In 2D are nodes are now a grid\nX,Y = np.meshgrid(x,y)\n\n\na = -3\nb = 3\nV = m = (b-a)*np.random.random([len(x)*len(y),2]) + a\n\n# normalise to unit vectors\nfor i in range(len(V[:,0])):\n    norm = np.linalg.norm(V[i,:])\n    V[i,:] = V[i,:]/norm\n\n# scale the unit vectors to look nicer in the plot\nV = V*0.4\n\n# loop variable\np = 0\n\nplt.plot(X,Y,'o',color = 'k')\n\nfor i in range(len(x)):\n    for j in range(len(y)):\n    \n        plt.arrow(x[i],y[j],V[p,0],V[p,1], color = \"r\", length_includes_head= True, head_width = 0.05)\n        p = p +1\n\n\nplt.title(\"Generate a random grid of vectors\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Plot"
  },
  {
    "objectID": "Posters_Talks/MathOnc23/index.html",
    "href": "Posters_Talks/MathOnc23/index.html",
    "title": "Mathematical modelling of interacting sub-populations in glioblastoma",
    "section": "",
    "text": "Abstract\nOne of the major challenges in successfully treating glioblastoma (GBM) is the significant heterogeneity in cellular composition observed within and between patients. Recent single cell transcriptomics suggests there can be as many as eighteen distinct cell types in a single tumour (Al-Dalahmah et al. 2021). Furthermore, advances in cellular deconvolution techniques, such as CIBERSORTx, allow us to accurately determine the cellular composition of imaged localised biopsies from bulk RNA-Seq (Steen et al. 2020) . Understanding this heterogeneity and how the complex interactions between cellular populations impacts the progression of GBM may lead to novel treatments which exploit the unique cellular composition within individual tumours. We group these eighteen cell types into sub-populations, e.g., glioma, immune, astrocyte, then attempt to learn the dynamics of these sub-populations by considering various interacting ODE/PDE models. Typically, a GBM patient will have biopsies taken at most twice, as well as only a handful of MRI scans. Therefore, the number of temporal data points to fit any model to are very limited. Thus, we apply trajectory inference methods, such as Monocle, to biopsy data, which allows us to order samples via pseudotime, an arbitrary unit of progress akin to real time (Trapnell et al. 2014). We illustrate our modelling approach with a simplified two species Lotka-Volterra style competition model.\n\n\n\n\n\nReferences\n\nAl-Dalahmah, Osama, Michael G. Argenziano, Adithya Kannan, Aayushi Mahajan, Julia Furnari, Fahad Paryani, Deborah Boyett, et al. 2021. “Re-Convolving the Compositional Landscape of Primary and Recurrent Glioblastoma Reveals Prognostic and Targetable Tissue States.” http://dx.doi.org/10.1101/2021.07.06.451295.\n\n\nSteen, Chloé B., Chih Long Liu, Ash A. Alizadeh, and Aaron M. Newman. 2020. “Profiling Cell Type Abundance and Expression in Bulk Tissues with CIBERSORTx.” In, 135–57. Springer US. https://doi.org/10.1007/978-1-0716-0301-7_7.\n\n\nTrapnell, Cole, Davide Cacchiarelli, Jonna Grimsby, Prapti Pokharel, Shuqiang Li, Michael Morse, Niall J Lennon, Kenneth J Livak, Tarjei S Mikkelsen, and John L Rinn. 2014. “The Dynamics and Regulators of Cell Fate Decisions Are Revealed by Pseudotemporal Ordering of Single Cells.” Nature Biotechnology 32 (4): 381–86. https://doi.org/10.1038/nbt.2859."
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "My CV",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The Mathematics of Perlin noise\n\n\n\n\n\n\nPython\n\n\nMaths\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nNicholas Harbour\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Conferences.html",
    "href": "Conferences.html",
    "title": "Conerences, posters and talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author(s)\n        \n     \n  \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor(s)\n\n\nConference\n\n\nCategories\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nCCRG-01. INFERENCE OF CELL CYCLE REGULATION BETWEEN GLIOBLASTOMA SUBPOPULATIONS IN VIVO TO DRIVE COMPUTATIONAL AND MATHEMATICAL MODELS OF THE CANCER COMPLEX SYSTEM\n\n\nNicholas Harbour, Lee Curtin, Sebastian Velez, Michael Chappell, Matthew Hubbard, Osama Al-Dalahmah, Peter Canoll, Markus Owen, Kristin Swanson\n\n\nSociety of Neuro Oncology 28th annual meeting\n\n\nMathOnco, Bioinformatics, Published Abstract\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nMathematical modelling of interacting sub-populations in glioblastoma\n\n\nNicholas Harbour\n\n\nMathematical oncology meeting\n\n\nMathOnco, Bioinformatics, Pseudotime, GBM\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicholas Harbour",
    "section": "",
    "text": "Bio\nI am a PhD student at the university of Nottingham, based in the Centre for Mathematical Medicine and Biology (CMMB). My research focuses on using mathematical modelling, bioinformatics, data science and machine learning to better understand cancer and improve treatment outcomes."
  },
  {
    "objectID": "Posters_Talks/SNO23/index.html",
    "href": "Posters_Talks/SNO23/index.html",
    "title": "CCRG-01. INFERENCE OF CELL CYCLE REGULATION BETWEEN GLIOBLASTOMA SUBPOPULATIONS IN VIVO TO DRIVE COMPUTATIONAL AND MATHEMATICAL MODELS OF THE CANCER COMPLEX SYSTEM",
    "section": "",
    "text": "Abstract\nGlioblastoma (GBM) is the most aggressive and most common primary malignant brain tumor in adults, with a poor median survival time of 15 months. One of the key challenges in treating GBM is its highly heterogeneous nature, with multiple distinct subtypes that have been shown to occur on both inter- and intra-patient levels. Three main classifications, known as classical/proliferative, mesenchymal and proneural have become commonly demonstrated phenotypes. The cell cycle is a fundamental and highly conserved process that controls faithful division of cells; dysregulation of the cell cycle is known to be a key driver in many cancers. However, how the cell cycle is differently regulated between these subtypes has not been well classified in vivo. We investigate these three GBM subtypes using a recently published single nucleus RNAseq (snRNAseq) data set. We compare cell cycle regulation/dysregulation among these three subtypes using Tricycle, an R/Bioconductor package that utilises dimension reduction via principal component analysis and transfer learning to infer cell cycle position from any snRNAseq data set. We find that the classical GBM subtype has the highest proportion of actively dividing cells (cells in: S/G2/M phases), while the mesenchymal and proneural subtypes have a very low proportion of actively dividing cells. This supports the idea of a proliferation-migration dichotomy between GBM subtypes. We use this proportion of actively proliferating cells to calibrate a minimal spatiotemporal mathematical model for GBM tumor growth that accounts for the differences in cell cycle regulation between these three GBM subtypes."
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author(s)\n        \n     \n  \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor(s)\n\n\nJournal\n\n\nCategories\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nCCRG-01. INFERENCE OF CELL CYCLE REGULATION BETWEEN GLIOBLASTOMA SUBPOPULATIONS IN VIVO TO DRIVE COMPUTATIONAL AND MATHEMATICAL MODELS OF THE CANCER COMPLEX SYSTEM\n\n\nNicholas Harbour, Lee Curtin, Sebastian Velez, Michael Chappell, Matthew Hubbard, Osama Al-Dalahmah, Peter Canoll, Markus Owen, Kristin Swanson\n\n\nNeuro-Oncolgy, Volume 25, Issue Supplement 5\n\n\nMathOnco, Bioinformatics, Published Abstract\n\n\n\n\n\n\n\n\nNo matching items"
  }
]